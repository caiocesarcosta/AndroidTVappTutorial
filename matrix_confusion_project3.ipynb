{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOy6U/Rhf1F6LUYFYJtEkP0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caiocesarcosta/AndroidTVappTutorial/blob/master/matrix_confusion_project3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82R_7aMYGCfN",
        "outputId": "2e574f82-6396-4b22-8e98-c8f6815ca5d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Found 602 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100 images belonging to 2 classes.\n",
            "Imagens de treino: 602\n",
            "Imagens de validação: 100\n",
            "Classes: {'cats': 0, 'dogs': 1}\n",
            "Class names: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1]\n",
            "Training Batch 1 - batch_x shape: (32, 150, 150, 3), batch_y shape: (32,), classes: [1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
            " 1. 0. 1. 1. 1. 0. 1. 0.]\n",
            "Training Batch 2 - batch_x shape: (32, 150, 150, 3), batch_y shape: (32,), classes: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0.\n",
            " 1. 1. 1. 1. 1. 0. 0. 1.]\n",
            "Training Batch 3 - batch_x shape: (32, 150, 150, 3), batch_y shape: (32,), classes: [0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 0. 0. 1. 0. 1. 1.]\n",
            "Validation Batch 1 - batch_x shape: (32, 150, 150, 3), batch_y shape: (32,), classes: [1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 0. 1. 1. 0. 0. 1.]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd # Importa o pandas com o alias 'pd'\n",
        "# Importa as funções roc_curve e auc explicitamente\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Classe MyCallback\n",
        "class MyCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(f\"\\nEpoch {epoch+1}/{self.epochs} - Precisão de Treino: {logs['accuracy']:.4f}, Perda de Treino: {logs['loss']:.4f}, Precisão de Validação: {logs['val_accuracy']:.4f}, Perda de Validação: {logs['val_loss']:.4f}\")\n",
        "\n",
        "# Classe Principal\n",
        "class TransferLearningBinary:\n",
        "    def __init__(self, train_data_dir, validation_data_dir, nb_train_samples, nb_validation_samples, epochs, batch_size):\n",
        "        self.__train_data_dir = train_data_dir\n",
        "        self.__validation_data_dir = validation_data_dir\n",
        "        self.__nb_train_samples = nb_train_samples\n",
        "        self.__nb_validation_samples = nb_validation_samples\n",
        "        self.__epochs = epochs\n",
        "        self.__batch_size = batch_size\n",
        "        self.__img_width, self.__img_height = 150, 150\n",
        "        self.__model = self._getModelSequential()\n",
        "        self._compileModel()\n",
        "\n",
        "    def _validateImage(self):\n",
        "        drive.mount('/content/drive')\n",
        "        try:\n",
        "            # Vamos tentar ler um arquivo de cada classe para garantir que elas estão presentes\n",
        "            cat_path = f'{self.__train_data_dir}/cats/0.jpg'\n",
        "            dog_path = f'{self.__train_data_dir}/dogs/0.jpg'\n",
        "\n",
        "            # Verifica se o arquivo existe, caso não, o programa é interrompido\n",
        "            if not os.path.exists(cat_path):\n",
        "               print(f\"ERRO: Imagem de teste não encontrada: {cat_path}, Verifique o caminho e se há imagens nas pastas train/cats\")\n",
        "               exit()\n",
        "            if not os.path.exists(dog_path):\n",
        "               print(f\"ERRO: Imagem de teste não encontrada: {dog_path}, Verifique o caminho e se há imagens nas pastas train/dogs\")\n",
        "               exit()\n",
        "\n",
        "            # leitura da imagem de exemplo, que será utilizada para obter as dimensões\n",
        "            img = image.load_img(cat_path)\n",
        "            img = image.img_to_array(img)\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "            input_shape = (self.__img_width, self.__img_height, 3) if tf.keras.backend.image_data_format() == 'channels_last' else (3, self.__img_width, self.__img_height)\n",
        "            return input_shape\n",
        "        except Exception as e:\n",
        "            print(f\"ERRO ao carregar imagem: {e}\")\n",
        "            exit()\n",
        "\n",
        "    def _getModelSequential(self):\n",
        "        input_shape = self._validateImage()\n",
        "        return Sequential([\n",
        "            Conv2D(32, (3, 3), input_shape=input_shape),\n",
        "            Activation('relu'),\n",
        "            MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "            Conv2D(64, (3, 3)), # Adicionando mais uma camada conv\n",
        "            Activation('relu'),\n",
        "            MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "            Conv2D(128, (3, 3)), # Adicionando mais uma camada conv\n",
        "            Activation('relu'),\n",
        "            MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "            Flatten(),\n",
        "            Dense(128), # Aumentando o tamanho da camada densa\n",
        "            Activation('relu'),\n",
        "            Dropout(0.5),\n",
        "            Dense(1),\n",
        "            Activation('sigmoid')\n",
        "        ])\n",
        "\n",
        "    def _compileModel(self):\n",
        "        self.__model.compile(loss='binary_crossentropy',\n",
        "                              optimizer='adam', # Adicionando Adam como otimizador\n",
        "                              metrics=['accuracy'])\n",
        "\n",
        "    def _initTrain(self):\n",
        "        train_datagen = ImageDataGenerator(\n",
        "            rescale=1. / 255,\n",
        "            shear_range=0.2,\n",
        "            zoom_range=0.2,\n",
        "            horizontal_flip=True)\n",
        "\n",
        "        test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "        self.__train_generator = train_datagen.flow_from_directory(\n",
        "            self.__train_data_dir,\n",
        "            target_size=(self.__img_width, self.__img_height),\n",
        "            batch_size=self.__batch_size,\n",
        "            class_mode='binary')\n",
        "\n",
        "        self.__validation_generator = test_datagen.flow_from_directory(\n",
        "            self.__validation_data_dir,\n",
        "            target_size=(self.__img_width, self.__img_height),\n",
        "            batch_size=self.__batch_size,\n",
        "            class_mode='binary')\n",
        "\n",
        "        print(f\"Imagens de treino: {len(self.__train_generator.filenames)}\")\n",
        "        print(f\"Imagens de validação: {len(self.__validation_generator.filenames)}\")\n",
        "        print(f\"Classes: {self.__train_generator.class_indices}\")\n",
        "        print(f\"Class names: {self.__train_generator.classes}\")\n",
        "\n",
        "        # Print batch sample for debugging\n",
        "        for i in range(min(3, len(self.__train_generator))): # Limit to 3 batches\n",
        "            batch_x, batch_y = self.__train_generator[i]\n",
        "            print(f\"Training Batch {i+1} - batch_x shape: {batch_x.shape}, batch_y shape: {batch_y.shape}, classes: {batch_y}\")\n",
        "        for i in range(min(3, len(self.__validation_generator))): # Limit to 3 batches\n",
        "            batch_x, batch_y = self.__validation_generator[i]\n",
        "            print(f\"Validation Batch {i+1} - batch_x shape: {batch_x.shape}, batch_y shape: {batch_y.shape}, classes: {batch_y}\")\n",
        "\n",
        "\n",
        "    def _historyTrain(self):\n",
        "        callback = MyCallback()\n",
        "        callback.epochs = self.__epochs\n",
        "        history = self.__model.fit(\n",
        "            self.__train_generator,\n",
        "            steps_per_epoch=self.__nb_train_samples // self.__batch_size,\n",
        "            epochs=self.__epochs,\n",
        "            validation_data=self.__validation_generator,\n",
        "            validation_steps=self.__nb_validation_samples // self.__batch_size,\n",
        "            callbacks=[callback]\n",
        "        )\n",
        "\n",
        "        print(\"\\nTraining History:\")\n",
        "        for epoch in range(self.__epochs):\n",
        "            print(f\"  Época {epoch+1}: Precisão de Treino = {history.history['accuracy'][epoch]:.4f}, Perda de Treino = {history.history['loss'][epoch]:.4f}, Precisão de Validação = {history.history['val_accuracy'][epoch]:.4f}, Perda de Validação = {history.history['val_loss'][epoch]:.4f}\")\n",
        "\n",
        "    def _createMatrixConfusion(self):\n",
        "      predictions = self.__model.predict(self.__validation_generator)\n",
        "      predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
        "      true_classes = self.__validation_generator.classes\n",
        "      class_labels = ['Cachorros', 'Gatos'] # definindo a ordem das labels\n",
        "\n",
        "      # Imprime métricas de classificação\n",
        "      report = classification_report(true_classes, predicted_classes, target_names=class_labels, output_dict=True)\n",
        "      #print(\"\\nClassification Report:\")\n",
        "      #print(report)\n",
        "\n",
        "      # Plota a matriz de confusão\n",
        "      cm = confusion_matrix(true_classes, predicted_classes, labels=[0, 1])  #labels garante a ordem das classes\n",
        "      plt.figure(figsize=(8, 6))\n",
        "      sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "      plt.xlabel('Previsões')\n",
        "      plt.ylabel('Rótulos Verdadeiros')\n",
        "      plt.title('Matriz de Confusão')\n",
        "      plt.show()\n",
        "\n",
        "       # Extrai métricas e coloca em um DataFrame\n",
        "      metrics = {\n",
        "            'Acurácia': report['accuracy'],\n",
        "            'Precisão (Cachorros)': report['Cachorros']['precision'],\n",
        "            'Sensibilidade (Cachorros)': report['Cachorros']['recall'],\n",
        "            'F1-Score (Cachorros)': report['Cachorros']['f1-score'],\n",
        "            'Precisão (Gatos)': report['Gatos']['precision'],\n",
        "            'Sensibilidade (Gatos)': report['Gatos']['recall'],\n",
        "            'F1-Score (Gatos)': report['Gatos']['f1-score'],\n",
        "            'Erro': 1 - report['accuracy']\n",
        "      }\n",
        "\n",
        "      # Cria o DataFrame e o exibe como tabela\n",
        "      df_metrics = pd.DataFrame(metrics, index=[0])\n",
        "      print(df_metrics.T)\n",
        "\n",
        "      #Calcula a curva ROC e a AUC\n",
        "      fpr, tpr, thresholds = roc_curve(true_classes, predictions)\n",
        "      roc_auc = auc(fpr, tpr)\n",
        "      print(f\"Area Under the ROC Curve: {roc_auc:.2f}\")\n",
        "\n",
        "    def _saveModel(self):\n",
        "        self.__model.save_weights('first_try.weights.h5')\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Orchestrates the complete model training process.\n",
        "        \"\"\"\n",
        "        self._initTrain()\n",
        "        self._historyTrain()\n",
        "        self._createMatrixConfusion()\n",
        "        self._saveModel()\n",
        "\n",
        "\n",
        "# Exemplo de uso:\n",
        "train_dir = '/content/drive/MyDrive/cats-dogs-projeto1/data/train'\n",
        "val_dir = '/content/drive/MyDrive/cats-dogs-projeto1/data/validation'\n",
        "\n",
        "transfer_learning = TransferLearningBinary(train_dir, val_dir, 602, 100, 1, 32)\n",
        "transfer_learning.train()"
      ]
    }
  ]
}